{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00002-f0819b38-c012-4fd9-b1a0-40b32986cb21",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 318.171875,
    "deepnote_cell_type": "code",
    "id": "wzEwkr3SoyLh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "%load_ext autoreload\n",
    "%matplotlib widget\n",
    "\n",
    "import sys, os, pickle, pdb, shutil, re, math\n",
    "from copy import deepcopy, copy\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd, numpy as np, torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from optimizing_for_explainability.utils import lime_fit, sample_around, CatLayer, finite_diff\n",
    "\n",
    "paths = [Path(\"\").parent.absolute() / \"shap\", Path(\"\").parent.absolute() / \"shap_original\"]\n",
    "for path in paths:\n",
    "    if str(path) not in sys.path:\n",
    "        sys.path.insert(0, str(path))\n",
    "import shap, shap_original\n",
    "\n",
    "DTYPE, DEVICE = torch.float32, torch.device(\"cuda\")\n",
    "TOPTS = dict(dtype=DTYPE, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00002-f0819b38-c012-4fd9-b1a0-40b32986cb21",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 318.171875,
    "deepnote_cell_type": "code",
    "id": "wzEwkr3SoyLh"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(\"\") / \"data\" / \"compas\" / \"cox-violent-parsed_filt.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "\n",
    "# Filter out entries with no indication of recidivism or no compass score\n",
    "df = df[df[\"is_recid\"] != -1]\n",
    "df = df[df[\"decile_score\"] != -1]\n",
    "\n",
    "# Rename recidivism column\n",
    "df[\"recidivism_within_2_years\"] = df[\"is_recid\"]\n",
    "\n",
    "# Make the COMPASS label column numeric (0 and 1), for use in our model\n",
    "df[\"COMPASS_determination\"] = np.where(df[\"score_text\"] == \"Low\", 0, 1)\n",
    "\n",
    "df = pd.get_dummies(df, columns=[\"sex\", \"race\"])\n",
    "\n",
    "# Get list of all columns from the dataset we will use for model input or output.\n",
    "input_features = [\n",
    "    \"sex_Female\",\n",
    "    \"sex_Male\",\n",
    "    \"age\",\n",
    "    \"race_African-American\",\n",
    "    \"race_Caucasian\",\n",
    "    \"race_Hispanic\",\n",
    "    \"race_Native American\",\n",
    "    \"race_Other\",\n",
    "    \"priors_count\",\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "]\n",
    "\n",
    "to_keep = input_features + [\"recidivism_within_2_years\", \"COMPASS_determination\"]\n",
    "\n",
    "to_remove = [col for col in df.columns if col not in to_keep]\n",
    "df = df.drop(columns=to_remove)\n",
    "\n",
    "input_columns = df.columns.tolist()\n",
    "labels = df[\"COMPASS_determination\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00004-de54ccf0-cc72-441f-9628-7a908132a348",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 246.15625,
    "deepnote_cell_type": "code",
    "id": "U0ZfePT1rTmZ"
   },
   "outputs": [],
   "source": [
    "# Create data structures needing for training and testing.\n",
    "# The training data doesn't contain the column we are predicting,\n",
    "# 'COMPASS_determination', or the column we are using for evaluation of our\n",
    "# trained model, 'recidivism_within_2_years'.\n",
    "df_for_training = df.drop(columns=[\"COMPASS_determination\", \"recidivism_within_2_years\"])\n",
    "train_size = int(len(df_for_training) * 0.8)\n",
    "\n",
    "train_data = df_for_training[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "test_data = df_for_training[train_size:]\n",
    "test_labels = labels[train_size:]\n",
    "\n",
    "test_data_with_labels = df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr = train_data.values, train_labels.values\n",
    "Xts, Yts = test_data.values, test_labels.values\n",
    "MU, STD = np.mean(Xtr, -2), np.std(Xtr, -2)\n",
    "normalize_fn = lambda x: (x - MU[None, ...]) / STD[None, ...]\n",
    "Xtr = normalize_fn(Xtr)\n",
    "Xts = normalize_fn(Xts)\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(*[Xtr, Ytr])), batch_size=1024, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(*[Xts, Yts])), batch_size=1024, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00005-62743ad9-fb8b-453d-9b8e-f601cd11e0da",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 264.15625,
    "deepnote_cell_type": "code",
    "id": "5T2XThgosWX-"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "# This is the size of the array we'll be feeding into our model for each example\n",
    "input_size = len(train_data.iloc[0])\n",
    "\n",
    "RACE_IDX = [i for (i, z) in enumerate(train_data.columns) if re.match(r\"race_.*\", z) is not None]\n",
    "\n",
    "# activation = torch.nn.ReLU()\n",
    "activation = torch.nn.Softplus(1e1)\n",
    "\n",
    "\n",
    "def generate_model():\n",
    "    model = (\n",
    "        torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 128),\n",
    "            copy(activation),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            copy(activation),\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        .to(DTYPE)\n",
    "        .to(DEVICE)\n",
    "    )\n",
    "    loss_obj = torch.nn.BCELoss()\n",
    "    lam = 1e-3\n",
    "    loss_fn = lambda Yp, Y: loss_obj(Yp, Y) + sum(lam * torch.sum(param**2) / 2 for param in model.parameters())\n",
    "\n",
    "    x0 = torch.as_tensor(Xtr[torch.randint(0, Xtr.shape[0], size=(1000,)), :], device=DEVICE, dtype=DTYPE)\n",
    "    Xs = sample_around(x0, torch.tensor(STD, device=DEVICE, dtype=DTYPE), N=int(1e2), alf=1e-2)\n",
    "    Xs = Xs.transpose(0, 1)\n",
    "    loss_fn_ = loss_fn\n",
    "\n",
    "    def loss_fn(Yp, Y, penalize=True, penalty=\"exact\", gam=1e1):\n",
    "        ret = loss_fn_(Yp, Y)\n",
    "        if penalize:\n",
    "            Yp = model(Xs)\n",
    "            W, b = lime_fit(Xs, Yp)\n",
    "            if penalty == \"exact\":\n",
    "                ret = ret + gam * torch.mean(torch.norm(W[..., RACE_IDX, 0], dim=-1))\n",
    "            elif penalty == \"mse\":\n",
    "                ret = ret + gam * torch.mean(torch.norm(W[..., RACE_IDX, 0], dim=-1) ** 2)\n",
    "            elif penalty == \"super-exact\":\n",
    "                ret = ret + gam * torch.mean(torch.sqrt(torch.norm(W[..., RACE_IDX, 0], dim=-1)))\n",
    "            else:\n",
    "                raise ValueError(f\"penalty [{penalty}] is not supported\")\n",
    "        return ret\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.3)\n",
    "    return model, loss_fn, optimizer, scheduler, Xs\n",
    "\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    correct = 0\n",
    "    for X, Y in loader:\n",
    "        X, Y = X.to(DTYPE).to(DEVICE), Y.to(DEVICE)\n",
    "        Yp = model(X)\n",
    "        correct += torch.sum((Yp > 0.5).reshape(Y.shape) == Y).detach()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84577501dc6f4aa3914c3a002aca2101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalize = False\n",
      "0.22673706710338593\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ab9eac6e1b4b7a98f6d188210af73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalize = True\n",
      "Penalty = exact\n",
      "0.0029133434873074293\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9fc2c2eb2d4f539d94bf495a6b20f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalize = True\n",
      "Penalty = mse\n",
      "0.007907169871032238\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ea4de88ed24248b03c4b3055b617c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalize = True\n",
      "Penalty = super-exact\n",
      "0.0021191039122641087\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "accs, metrics = [], []\n",
    "for (penalize, penalty, gam) in [\n",
    "    (False, \"exact\", 1e1),\n",
    "    (True, \"mse\", 1e2),\n",
    "    (True, \"exact\", 1e1),\n",
    "    (True, \"super-exact\", 1.5e0),\n",
    "]:\n",
    "    model, loss_fn, optimizer, scheduler, Xs = generate_model()\n",
    "    shutil.rmtree(Path(\"\") / \"runs\")\n",
    "    writer = SummaryWriter()\n",
    "    rng = tqdm(range(int(10)))\n",
    "    for epoch in rng:\n",
    "        for (i, (X, Y)) in enumerate(train_loader):\n",
    "            X, Y = X.to(DTYPE).to(DEVICE), Y.to(DTYPE).to(DEVICE)\n",
    "            loss = loss_fn(model(X).reshape(Y.shape), Y, penalize=penalize, penalty=penalty, gam=gam)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # rng.set_description(f\"{loss.detach():.4e}\")\n",
    "            writer.add_scalar(\"loss/train\", float(loss), i + epoch * len(train_loader))\n",
    "            writer.add_scalar(\"step_size\", float(optimizer.param_groups[0][\"lr\"]), i + epoch * len(train_loader))\n",
    "            writer.flush()\n",
    "        scheduler.step()\n",
    "        # tqdm.write(f\"Accuracy = {1e2 * accuracy(model, test_loader):.3f}%\")\n",
    "        rng.set_description(\n",
    "            f\"Accuracy = (test = {1e2 * accuracy(model, test_loader):.3f}%,\"\n",
    "            + f\"train = {1e2 * accuracy(model, train_loader):.3f}%)\"\n",
    "        )\n",
    "        # tqdm.write(f\"Loss =     {loss_obj(model(X).reshape(Y.shape), Y):.5e}\")\n",
    "\n",
    "    W, b = lime_fit(Xs, model(Xs))\n",
    "    print(f\"Penalize = {penalize}\")\n",
    "    if penalize:\n",
    "        print(f\"Penalty = {penalty}\")\n",
    "    metric = torch.mean(torch.norm(W[..., RACE_IDX, 0], dim=-1))\n",
    "    print(float(metric))\n",
    "    accs.append(float(1e2 * accuracy(model, test_loader)))\n",
    "    metrics.append(float(metric))\n",
    "    print(\"#\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gam(gam):\n",
    "    model, loss_fn, optimizer, scheduler, Xs = generate_model()\n",
    "    shutil.rmtree(Path(\"\") / \"runs\")\n",
    "    writer = SummaryWriter()\n",
    "    rng = tqdm(range(int(10)))\n",
    "    for epoch in rng:\n",
    "        for (i, (X, Y)) in enumerate(train_loader):\n",
    "            X, Y = X.to(DTYPE).to(DEVICE), Y.to(DTYPE).to(DEVICE)\n",
    "            loss = loss_fn(model(X).reshape(Y.shape), Y, penalize=True, penalty=\"exact\", gam=gam)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(\"loss/train\", float(loss), i + epoch * len(train_loader))\n",
    "            writer.add_scalar(\"step_size\", float(optimizer.param_groups[0][\"lr\"]), i + epoch * len(train_loader))\n",
    "            writer.flush()\n",
    "        scheduler.step()\n",
    "        # tqdm.write(f\"Accuracy = {1e2 * accuracy(model, test_loader):.3f}%\")\n",
    "        rng.set_description(\n",
    "            f\"Accuracy = (test = {1e2 * accuracy(model, test_loader):.3f}%,\"\n",
    "            + f\"train = {1e2 * accuracy(model, train_loader):.3f}%)\"\n",
    "        )\n",
    "        # tqdm.write(f\"Loss =     {loss_obj(model(X).reshape(Y.shape), Y):.5e}\")\n",
    "\n",
    "    W, b = lime_fit(Xs, model(Xs))\n",
    "    print(f\"Penalize = {penalize}\")\n",
    "    if penalize:\n",
    "        print(f\"Penalty = {penalty}\")\n",
    "        print(f\"Gam = {gam}\")\n",
    "    metric = torch.mean(torch.norm(W[..., RACE_IDX, 0], dim=-1))\n",
    "    print(metric)\n",
    "    print(\"#\" * 80)\n",
    "    return metric\n",
    "\n",
    "\n",
    "gams = 10.0 ** np.linspace(-2, 2, 10)\n",
    "vals = [check_gam(gam) for gam in gams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing differentiating through Shapley Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP differentiability test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00007-7ae41274-6190-4adc-9032-8271acd18d87",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 102.15625,
    "deepnote_cell_type": "code",
    "id": "lI18CwYiQotq"
   },
   "outputs": [],
   "source": [
    "# Create a SHAP explainer by passing a subset of our training data\n",
    "model, loss_fn, optimizer, scheduler, Xs = generate_model()\n",
    "X_background = torch.as_tensor(Xtr[torch.randperm(Xtr.shape[0])[:100], ...], **TOPTS)\n",
    "X_test = torch.as_tensor(Xtr[torch.randperm(Xtr.shape[0])[:10], ...], **TOPTS)\n",
    "explainer = shap.DeepExplainer(model, X_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "00007-7ae41274-6190-4adc-9032-8271acd18d87",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 102.15625,
    "deepnote_cell_type": "code",
    "id": "lI18CwYiQotq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0073, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vals1 = explainer.shap_values(X_test)\n",
    "\n",
    "explainer_original = shap_original.DeepExplainer(deepcopy(model), X_background)\n",
    "vals2 = torch.as_tensor(explainer_original.shap_values(X_test), **TOPTS)\n",
    "\n",
    "print(torch.norm(vals1 - vals2) / torch.norm(vals1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5033e-06,  7.4467e-08, -3.2063e-07,  ...,  7.6408e-08,\n",
      "          1.4895e-08,  5.8486e-08],\n",
      "        [-9.7314e-04,  4.6529e-08,  1.4900e-07,  ..., -6.3004e-09,\n",
      "         -1.2282e-09,  1.9573e-09],\n",
      "        [ 6.3739e-05,  2.4759e-07, -1.7470e-06,  ...,  2.9007e-07,\n",
      "          5.6548e-08,  2.2286e-07],\n",
      "        ...,\n",
      "        [ 1.4102e-04,  1.7241e-08, -8.3069e-08,  ...,  2.4468e-08,\n",
      "          4.7699e-09,  2.1345e-08],\n",
      "        [-5.7081e-06, -1.2926e-08,  2.7734e-07,  ..., -4.1700e-08,\n",
      "         -8.1292e-09, -3.5948e-08],\n",
      "        [-5.5522e-05,  2.3477e-08, -2.0407e-07,  ...,  3.3517e-08,\n",
      "          6.5339e-09,  2.6414e-08]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "J2 = torch.autograd.grad(explainer.shap_values(X_test).reshape(-1)[0], next(model.parameters()))[0]\n",
    "print(J2)\n",
    "#f = lambda x: explainer.shap_values(x).detach().reshape(-1)[0]\n",
    "#J = finite_diff(f, X_test, 1e-5)\n",
    "#X_test.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-input test (we probably won't need this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-7ae41274-6190-4adc-9032-8271acd18d87",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 102.15625,
    "deepnote_cell_type": "code",
    "id": "lI18CwYiQotq"
   },
   "outputs": [],
   "source": [
    "model, loss_fn, optimizer, scheduler, Xs = generate_model()\n",
    "X_background = torch.as_tensor(Xtr[torch.randperm(Xtr.shape[0])[:100], ...], **TOPTS)\n",
    "X_test = torch.as_tensor(Xtr[torch.randperm(Xtr.shape[0])[:10], ...], **TOPTS)\n",
    "Xb1, Xb2 = X_background[..., :3], X_background[..., 3:]\n",
    "Xt1, Xt2 = X_test[..., :3], X_test[..., 3:]\n",
    "\n",
    "explainer = shap.DeepExplainer(deepcopy(model2), [Xb1, Xb2])\n",
    "vals1 = explainer.shap_values([Xt1, Xt2])\n",
    "\n",
    "explainer_original = shap_original.DeepExplainer(deepcopy(model2), [Xb1, Xb2])\n",
    "vals2 = explainer_original.shap_values([Xt1, Xt2])\n",
    "\n",
    "#print(torch.norm(vals1 - vals2) / torch.norm(vals1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WIT COMPAS with SHAP",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "f3c3b30d-7a29-47a3-a6e6-89fe2e1d8b52",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
