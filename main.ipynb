{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00002-f0819b38-c012-4fd9-b1a0-40b32986cb21",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 318.171875,
    "deepnote_cell_type": "code",
    "id": "wzEwkr3SoyLh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "%load_ext autoreload\n",
    "%matplotlib widget\n",
    "\n",
    "import sys, os, pickle, pdb, shutil, re, math\n",
    "from copy import deepcopy, copy\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Union, Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import psutil\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd, numpy as np, torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from optimizing_for_explainability.penalty_functions import mse_penalty, exact_penalty, super_exact_penalty\n",
    "from optimizing_for_explainability.penalties import LIME_penalty, SHAP_penalty\n",
    "\n",
    "# import the shap module\n",
    "paths = [Path(\"\").parent.absolute() / \"shap\", Path(\"\").parent.absolute() / \"shap_original\"]\n",
    "for path in paths:\n",
    "    if str(path) not in sys.path:\n",
    "        sys.path.insert(0, str(path))\n",
    "import shap, shap_original\n",
    "\n",
    "# set the global dtype and device to work with\n",
    "DTYPE, DEVICE = torch.float32, torch.device(\"cuda\")\n",
    "TOPTS = dict(dtype=DTYPE, device=DEVICE)  # tensor options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00002-f0819b38-c012-4fd9-b1a0-40b32986cb21",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 318.171875,
    "deepnote_cell_type": "code",
    "id": "wzEwkr3SoyLh"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "df = pd.read_csv(Path(\"\") / \"data\" / \"compas\" / \"cox-violent-parsed_filt.csv\")\n",
    "\n",
    "# Filter out entries with no indication of recidivism or no compass score\n",
    "df = df[df[\"is_recid\"] != -1]\n",
    "\n",
    "df = df[df[\"decile_score\"] != -1]\n",
    "# Rename recidivism column\n",
    "df[\"recidivism_within_2_years\"] = df[\"is_recid\"]\n",
    "\n",
    "# Make the COMPASS label column numeric (0 and 1), for use in our model\n",
    "df[\"COMPASS_determination\"] = np.where(df[\"score_text\"] == \"Low\", 0, 1)\n",
    "\n",
    "df = pd.get_dummies(df, columns=[\"sex\", \"race\"])\n",
    "\n",
    "# Get list of all columns from the dataset we will use for model input or output.\n",
    "input_features = [\n",
    "    \"sex_Female\",\n",
    "    \"sex_Male\",\n",
    "    \"age\",\n",
    "    \"race_African-American\",\n",
    "    \"race_Caucasian\",\n",
    "    \"race_Hispanic\",\n",
    "    \"race_Native American\",\n",
    "    \"race_Other\",\n",
    "    \"priors_count\",\n",
    "    \"juv_fel_count\",\n",
    "    \"juv_misd_count\",\n",
    "    \"juv_other_count\",\n",
    "]\n",
    "\n",
    "to_keep = input_features + [\"recidivism_within_2_years\", \"COMPASS_determination\"]\n",
    "\n",
    "to_remove = [col for col in df.columns if col not in to_keep]\n",
    "df = df.drop(columns=to_remove)\n",
    "\n",
    "input_columns = df.columns.tolist()\n",
    "labels = df[\"COMPASS_determination\"]\n",
    "\n",
    "# Create data structures needing for training and testing.\n",
    "# The training data doesn't contain the column we are predicting,\n",
    "# 'COMPASS_determination', or the column we are using for evaluation of our\n",
    "# trained model, 'recidivism_within_2_years'.\n",
    "df_for_training = df.drop(columns=[\"COMPASS_determination\", \"recidivism_within_2_years\"])\n",
    "train_size = int(len(df_for_training) * 0.8)\n",
    "\n",
    "train_data = df_for_training[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "test_data = df_for_training[train_size:]\n",
    "test_labels = labels[train_size:]\n",
    "\n",
    "test_data_with_labels = df[train_size:]\n",
    "\n",
    "# extract the data into datasets and data loaders ##############################\n",
    "Xtr, Ytr = train_data.values, train_labels.values\n",
    "Xts, Yts = test_data.values, test_labels.values\n",
    "MU, STD = np.mean(Xtr, -2), np.std(Xtr, -2)\n",
    "normalize_fn = lambda x: (x - MU[None, ...]) / STD[None, ...]\n",
    "Xtr = normalize_fn(Xtr)\n",
    "Xts = normalize_fn(Xts)\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(*[Xtr, Ytr])), batch_size=1024, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(*[Xts, Yts])), batch_size=1024, num_workers=8)\n",
    "\n",
    "# define protected variables\n",
    "RACE_IDX = [i for (i, z) in enumerate(train_data.columns) if re.match(r\"race_.*\", z) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00005-62743ad9-fb8b-453d-9b8e-f601cd11e0da",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 264.15625,
    "deepnote_cell_type": "code",
    "id": "5T2XThgosWX-"
   },
   "outputs": [],
   "source": [
    "def generate_model(config):\n",
    "    # This is the size of the array we'll be feeding into our model for each example\n",
    "    input_size = len(train_data.iloc[0])\n",
    "    activation = torch.nn.Softplus(1e1)\n",
    "\n",
    "    model = (\n",
    "        torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 128),\n",
    "            copy(activation),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            copy(activation),\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        .to(DTYPE)\n",
    "        .to(DEVICE)\n",
    "    )\n",
    "    loss_obj = torch.nn.BCELoss()\n",
    "\n",
    "    def loss_fn(Yp, Y, **kw):\n",
    "        kw = dict(config, **kw)\n",
    "        loss = loss_obj(Yp, Y)\n",
    "        loss = loss + sum(config[\"lam\"] * torch.sum(param**2) / 2 for param in model.parameters())\n",
    "        return loss\n",
    "\n",
    "    def cstr_fn(**kw):\n",
    "        kw = dict(config, **kw)\n",
    "        penalty_fn = lambda x: exact_penalty(x, kw.get(\"gam\", None))\n",
    "\n",
    "        if kw.get(\"method\", \"\").lower() == \"lime\":\n",
    "            loss_penalty = LIME_penalty(\n",
    "                RACE_IDX,\n",
    "                model,\n",
    "                train_loader.dataset,\n",
    "                STD,\n",
    "                penalty_fn=penalty_fn,\n",
    "                test_samples=kw[\"test_samples\"],\n",
    "                bg_samples=kw[\"bg_samples\"],\n",
    "                sample_std=kw[\"sample_std\"],\n",
    "            )\n",
    "        elif kw.get(\"method\", \"\").lower() == \"shap\":\n",
    "            loss_penalty = SHAP_penalty(\n",
    "                RACE_IDX,\n",
    "                model,\n",
    "                train_loader.dataset,\n",
    "                penalty_fn=penalty_fn,\n",
    "                test_samples=kw[\"test_samples\"],\n",
    "                bg_samples=kw[\"bg_samples\"],\n",
    "            )\n",
    "        return loss_penalty\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.3)\n",
    "    return model, loss_fn, cstr_fn, optimizer, scheduler\n",
    "\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    correct = 0\n",
    "    for X, Y in loader:\n",
    "        X, Y = X.to(DTYPE).to(DEVICE), Y.to(DEVICE)\n",
    "        Yp = model(X)\n",
    "        correct += torch.sum((Yp > 0.5).reshape(Y.shape) == Y).detach()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714d8f06d6ed4ac3b207155bb8aa02d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accs, metrics = [], []\n",
    "for (penalize, penalty, gam) in [\n",
    "    # (False, \"exact\", 1e1),\n",
    "    # (True, \"mse\", 1e2),\n",
    "    (True, \"exact\", 1e1),\n",
    "    # (True, \"super-exact\", 1.5e0),\n",
    "]:\n",
    "    #config = dict(test_samples=int(1e0), bg_samples=int(1e2), method=\"SHAP\", lam=1e-3, gam=1e0)\n",
    "    config = dict(test_samples=int(1e2), bg_samples=int(1e3), sample_std=1e-2, method=\"LIME\", lam=1e-3, gam=1e1)\n",
    "    model, loss_fn, cstr_fn, optimizer, scheduler = generate_model(config)\n",
    "    shutil.rmtree(Path(\"\") / \"runs\")\n",
    "    writer = SummaryWriter()\n",
    "    rng = tqdm(range(int(50)))\n",
    "    for epoch in rng:\n",
    "        for (i, (X, Y)) in enumerate(train_loader):\n",
    "            X, Y = X.to(DTYPE).to(DEVICE), Y.to(DTYPE).to(DEVICE)  # sample a batch\n",
    "            optimizer.zero_grad()  # zero gradients\n",
    "            cstr_val, grads = cstr_fn()\n",
    "            for (grad, param) in zip(grads, model.parameters()):\n",
    "                param.grad = grad\n",
    "            loss = loss_fn(model(X).reshape(Y.shape), Y)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(\"loss/train\", float(loss), i + epoch * len(train_loader))\n",
    "            writer.add_scalar(\"step_size\", float(optimizer.param_groups[0][\"lr\"]), i + epoch * len(train_loader))\n",
    "            writer.add_scalar(\"penalty_metric\", float(cstr_val), i + epoch * len(train_loader))\n",
    "            writer.flush()\n",
    "        scheduler.step()\n",
    "        rng.set_description(\n",
    "            f\"Accuracy = (test = {1e2 * accuracy(model, test_loader):.3f}%,\"\n",
    "            + f\"train = {1e2 * accuracy(model, train_loader):.3f}%)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Accuracy vs Penalty Strength Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gam(gam):\n",
    "    model, loss_fn, optimizer, scheduler, Xs = generate_model()\n",
    "    shutil.rmtree(Path(\"\") / \"runs\")\n",
    "    writer = SummaryWriter()\n",
    "    rng = tqdm(range(int(10)))\n",
    "    for epoch in rng:\n",
    "        for (i, (X, Y)) in enumerate(train_loader):\n",
    "            X, Y = X.to(DTYPE).to(DEVICE), Y.to(DTYPE).to(DEVICE)\n",
    "            loss = loss_fn(model(X).reshape(Y.shape), Y, penalize=True, penalty=\"exact\", gam=gam)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(\"loss/train\", float(loss), i + epoch * len(train_loader))\n",
    "            writer.add_scalar(\"step_size\", float(optimizer.param_groups[0][\"lr\"]), i + epoch * len(train_loader))\n",
    "            writer.flush()\n",
    "        scheduler.step()\n",
    "        # tqdm.write(f\"Accuracy = {1e2 * accuracy(model, test_loader):.3f}%\")\n",
    "        rng.set_description(\n",
    "            f\"Accuracy = (test = {1e2 * accuracy(model, test_loader):.3f}%,\"\n",
    "            + f\"train = {1e2 * accuracy(model, train_loader):.3f}%)\"\n",
    "        )\n",
    "        # tqdm.write(f\"Loss =     {loss_obj(model(X).reshape(Y.shape), Y):.5e}\")\n",
    "\n",
    "    W, b = lime_fit(Xs, model(Xs))\n",
    "    print(f\"Penalize = {penalize}\")\n",
    "    if penalize:\n",
    "        print(f\"Penalty = {penalty}\")\n",
    "        print(f\"Gam = {gam}\")\n",
    "    metric = torch.mean(torch.norm(W[..., RACE_IDX, 0], dim=-1))\n",
    "    print(metric)\n",
    "    print(\"#\" * 80)\n",
    "    return metric\n",
    "\n",
    "\n",
    "gams = 10.0 ** np.linspace(-2, 2, 10)\n",
    "vals = [check_gam(gam) for gam in gams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing differentiating through Shapley Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP differentiability test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00007-7ae41274-6190-4adc-9032-8271acd18d87",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 102.15625,
    "deepnote_cell_type": "code",
    "id": "lI18CwYiQotq"
   },
   "outputs": [],
   "source": [
    "# Create a SHAP explainer by passing a subset of our training data\n",
    "model, loss_fn, optimizer, scheduler, Xs = generate_model()\n",
    "X_background = torch.as_tensor(Xtr[torch.randperm(Xtr.shape[0])[:100], ...], **TOPTS)\n",
    "X_test = torch.as_tensor(Xtr[torch.randperm(Xtr.shape[0])[:10], ...], **TOPTS)\n",
    "explainer = shap.DeepExplainer(model, X_background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cell_id": "00007-7ae41274-6190-4adc-9032-8271acd18d87",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 102.15625,
    "deepnote_cell_type": "code",
    "id": "lI18CwYiQotq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0073, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vals1 = explainer.shap_values(X_test)\n",
    "\n",
    "explainer_original = shap_original.DeepExplainer(deepcopy(model), X_background)\n",
    "vals2 = torch.as_tensor(explainer_original.shap_values(X_test), **TOPTS)\n",
    "\n",
    "print(torch.norm(vals1 - vals2) / torch.norm(vals1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5033e-06,  7.4467e-08, -3.2063e-07,  ...,  7.6408e-08,\n",
      "          1.4895e-08,  5.8486e-08],\n",
      "        [-9.7314e-04,  4.6529e-08,  1.4900e-07,  ..., -6.3004e-09,\n",
      "         -1.2282e-09,  1.9573e-09],\n",
      "        [ 6.3739e-05,  2.4759e-07, -1.7470e-06,  ...,  2.9007e-07,\n",
      "          5.6548e-08,  2.2286e-07],\n",
      "        ...,\n",
      "        [ 1.4102e-04,  1.7241e-08, -8.3069e-08,  ...,  2.4468e-08,\n",
      "          4.7699e-09,  2.1345e-08],\n",
      "        [-5.7081e-06, -1.2926e-08,  2.7734e-07,  ..., -4.1700e-08,\n",
      "         -8.1292e-09, -3.5948e-08],\n",
      "        [-5.5522e-05,  2.3477e-08, -2.0407e-07,  ...,  3.3517e-08,\n",
      "          6.5339e-09,  2.6414e-08]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "J2 = torch.autograd.grad(explainer.shap_values(X_test).reshape(-1)[0], next(model.parameters()))[0]\n",
    "print(J2)\n",
    "# f = lambda x: explainer.shap_values(x).detach().reshape(-1)[0]\n",
    "# J = finite_diff(f, X_test, 1e-5)\n",
    "# X_test.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-input test (we probably won't need this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-7ae41274-6190-4adc-9032-8271acd18d87",
    "colab": {},
    "colab_type": "code",
    "deepnote_cell_height": 102.15625,
    "deepnote_cell_type": "code",
    "id": "lI18CwYiQotq"
   },
   "outputs": [],
   "source": [
    "model, loss_fn, optimizer, scheduler, Xs = generate_model()\n",
    "X_background = torch.as_tensor(Xtr[torch.randperm(Xtr.shape[0])[:100], ...], **TOPTS)\n",
    "X_test = torch.as_tensor(Xtr[torch.randperm(Xtr.shape[0])[:10], ...], **TOPTS)\n",
    "Xb1, Xb2 = X_background[..., :3], X_background[..., 3:]\n",
    "Xt1, Xt2 = X_test[..., :3], X_test[..., 3:]\n",
    "\n",
    "explainer = shap.DeepExplainer(deepcopy(model2), [Xb1, Xb2])\n",
    "vals1 = explainer.shap_values([Xt1, Xt2])\n",
    "\n",
    "explainer_original = shap_original.DeepExplainer(deepcopy(model2), [Xb1, Xb2])\n",
    "vals2 = explainer_original.shap_values([Xt1, Xt2])\n",
    "\n",
    "# print(torch.norm(vals1 - vals2) / torch.norm(vals1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WIT COMPAS with SHAP",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "f3c3b30d-7a29-47a3-a6e6-89fe2e1d8b52",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
